import vertexai
from vertexai.language_models import TextGenerationModel, ChatModel
from vertexai.generative_models import GenerativeModel
import os
from dotenv import load_dotenv

def test_ai_models_direct():
    """Test AI models directly using service account credentials"""
    load_dotenv()
    
    project_id = os.getenv('GOOGLE_CLOUD_PROJECT')
    location = os.getenv('VERTEXAI_LOCATION', 'us-central1')
    
    print(f"üîç Testing AI models directly")
    print(f"üìã Project: {project_id}")
    print(f"üìç Location: {location}")
    print("=" * 50)
    
    # Initialize Vertex AI
    try:
        vertexai.init(project=project_id, location=location)
        print("‚úÖ Vertex AI initialized successfully")
    except Exception as e:
        print(f"‚ùå Failed to initialize Vertex AI: {str(e)}")
        return
    
    # Test different model categories
    model_tests = [
        # Legacy text generation models (most likely to work)
        ("Legacy Text Models", [
            ("text-bison@002", "legacy_text"),
            ("text-bison@001", "legacy_text"),
            ("text-bison", "legacy_text")
        ]),
        
        # Legacy chat models
        ("Legacy Chat Models", [
            ("chat-bison@002", "legacy_chat"),
            ("chat-bison@001", "legacy_chat"),
            ("chat-bison", "legacy_chat")
        ]),
        
        # New Gemini models (might not be available)
        ("Gemini Models", [
            ("gemini-pro", "gemini"),
            ("gemini-1.0-pro", "gemini"),
            ("gemini-1.5-pro", "gemini"),
            ("gemini-1.5-flash", "gemini")
        ])
    ]
    
    working_models = []
    
    for category, models in model_tests:
        print(f"\nüß™ Testing {category}:")
        print("-" * 30)
        
        for model_name, model_type in models:
            try:
                print(f"  Testing {model_name}...", end=" ")
                
                if model_type == "legacy_text":
                    model = TextGenerationModel.from_pretrained(model_name)
                    response = model.predict("Hello", max_output_tokens=10)
                    if response and response.text:
                        print("‚úÖ WORKING")
                        working_models.append((model_name, model_type))
                    else:
                        print("‚ö†Ô∏è  NO RESPONSE")
                        
                elif model_type == "legacy_chat":
                    model = ChatModel.from_pretrained(model_name)
                    chat = model.start_chat()
                    response = chat.send_message("Hello", max_output_tokens=10)
                    if response and response.text:
                        print("‚úÖ WORKING")
                        working_models.append((model_name, model_type))
                    else:
                        print("‚ö†Ô∏è  NO RESPONSE")
                        
                elif model_type == "gemini":
                    model = GenerativeModel(model_name)
                    response = model.generate_content("Hello")
                    if response and response.text:
                        print("‚úÖ WORKING")
                        working_models.append((model_name, model_type))
                    else:
                        print("‚ö†Ô∏è  NO RESPONSE")
                        
            except Exception as e:
                error_msg = str(e).lower()
                if "not found" in error_msg or "does not exist" in error_msg:
                    print("‚ùå NOT FOUND")
                elif "permission" in error_msg or "access" in error_msg:
                    print("üîí NO ACCESS")
                elif "quota" in error_msg or "limit" in error_msg:
                    print("‚ö†Ô∏è  QUOTA EXCEEDED")
                else:
                    print(f"‚ùå ERROR: {str(e)[:50]}...")
    
    # Summary
    print("\n" + "="*50)
    print("üìä RESULTS SUMMARY")
    print("="*50)
    
    if working_models:
        print(f"‚úÖ Found {len(working_models)} working model(s):")
        for model_name, model_type in working_models:
            print(f"  ‚Ä¢ {model_name} ({model_type})")
        
        # Recommend the best model
        best_model = working_models[0]
        print(f"\nüéØ RECOMMENDED MODEL: {best_model[0]}")
        print(f"üìù Model Type: {best_model[1]}")
        
        # Create a simple test
        print(f"\nüß™ Testing recommended model with travel query...")
        try:
            if best_model[1] == "legacy_text":
                model = TextGenerationModel.from_pretrained(best_model[0])
                response = model.predict(
                    "Create a 1-day itinerary for Delhi with budget ‚Çπ2000",
                    max_output_tokens=200
                )
                print(f"‚úÖ Travel test successful!")
                print(f"üìù Sample response: {response.text[:100]}...")
                
            elif best_model[1] == "legacy_chat":
                model = ChatModel.from_pretrained(best_model[0])
                chat = model.start_chat()
                response = chat.send_message(
                    "Create a 1-day itinerary for Delhi with budget ‚Çπ2000",
                    max_output_tokens=200
                )
                print(f"‚úÖ Travel test successful!")
                print(f"üìù Sample response: {response.text[:100]}...")
                
            elif best_model[1] == "gemini":
                model = GenerativeModel(best_model[0])
                response = model.generate_content(
                    "Create a 1-day itinerary for Delhi with budget ‚Çπ2000"
                )
                print(f"‚úÖ Travel test successful!")
                print(f"üìù Sample response: {response.text[:100]}...")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Travel test failed: {str(e)[:100]}...")
        
        print(f"\nüöÄ NEXT STEPS:")
        print(f"1. Your app will automatically use: {best_model[0]}")
        print(f"2. Run: streamlit run app.py")
        print(f"3. Test the travel planner!")
        
    else:
        print("‚ùå No working models found!")
        print("\nüîß TROUBLESHOOTING:")
        print("1. Check if Vertex AI API is enabled in Google Cloud Console")
        print("2. Verify your service account has 'Vertex AI User' role")
        print("3. Ensure billing is enabled on your project")
        print("4. Try a different region in your .env file:")
        print("   VERTEXAI_LOCATION=us-east1")

if __name__ == "__main__":
    test_ai_models_direct()
